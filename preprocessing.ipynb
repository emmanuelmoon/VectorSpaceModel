{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4261e0c0-ed97-4db6-a183-5016d56573af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/emmanuel/miniconda3/envs/vsm/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: numpy in /home/emmanuel/miniconda3/envs/vsm/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: click in /home/emmanuel/miniconda3/envs/vsm/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/emmanuel/miniconda3/envs/vsm/lib/python3.12/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/emmanuel/miniconda3/envs/vsm/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /home/emmanuel/miniconda3/envs/vsm/lib/python3.12/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91229c2-0944-4a05-bf55-63c170ecf4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/emmanuel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "775b1f1c-4d02-495c-bdf8-3126e241cd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Stopword-List.txt', 'r')\n",
    "stop = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8a1830-4272-4610-bffa-cc294640d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"[\\w]+|[^-_\\w\\s()@#$%^&*+={[\\]};,<>./?~`\\\"]\"\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ac4afc3-be32-48fe-bdf8-c999cc83a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "docs = set()\n",
    "dic = {}\n",
    "punc = ['!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '+', '=', '{', '[', ']', '}', ':', ';', \"'\", '\"', ',', '<', '>', '.', '/', '?', '~', '`']\n",
    "\n",
    "for subdir, dirs, files in os.walk('ResearchPapers'):\n",
    "    for file in files:\n",
    "        with open(subdir + os.sep + file, 'r', encoding='cp1252') as txt:\n",
    "            # Extract document ID from the filename\n",
    "            doc = re.search('[0-9]*', file).group()\n",
    "            doc = int(doc)\n",
    "            docs.add(doc) # Add document ID to the set of docs\n",
    "            # Read and tokenize the text\n",
    "            tokens = tokenize(txt.read())\n",
    "            \n",
    "            # Process each token\n",
    "            for t in tokens:\n",
    "                if t not in stop and t not in punc:\n",
    "                    # Lowercase and stem the token\n",
    "                    term = ps.stem(t.lower())\n",
    "\n",
    "                    # Update the inverted index\n",
    "                    if term not in dic:\n",
    "                        dic[term] = defaultdict(int)\n",
    "                    dic[term][doc] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e520954-3089-479f-9334-e3281084f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a mapping from docs to indices in term-document matrix\n",
    "\n",
    "doc_list = list(docs)\n",
    "doc_map = dict()\n",
    "for i in range(len(doc_list)):\n",
    "    doc_map[doc_list[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1aaaaca-6922-42a3-b6a3-c8551878567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a mapping from dictionary terms to indices in term-document matrix\n",
    "\n",
    "dic_map = dict()\n",
    "keys = sorted(dic.keys())\n",
    "for i in range(len(keys)):\n",
    "    dic_map[keys[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc93f6b-2302-4698-abc1-481cdc540eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the matrix with respective term frequencies\n",
    "\n",
    "import numpy as np\n",
    "a = np.empty(shape=(len(docs), len(dic)))\n",
    "a.fill(0)\n",
    "for key1, value1 in dic.items():\n",
    "    for key2, value2 in value1.items():\n",
    "        a[doc_map[key2]][dic_map[key1]] = value2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8709ba3b-7db2-42e5-8b5d-9a57bee31f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.00021344 0.00096927 ... 0.00162425 0.         0.01196151]\n",
      " [0.         0.00012997 0.         ... 0.00427617 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.01034325 0.         0.        ]\n",
      " ...\n",
      " [0.         0.00090674 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.00212811 0.0027059  ... 0.00826861 0.         0.        ]\n",
      " [0.         0.00059461 0.         ... 0.00220191 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Multiply each entry with its respect inverse document frequency\n",
    "\n",
    "import math\n",
    "from numpy import array\n",
    "from numpy.linalg import norm\n",
    "idf_map = dict()\n",
    "for key1, value1 in dic.items():\n",
    "    idf = math.log10(20/len(value1.keys()))\n",
    "    idf_map[key1] = idf\n",
    "    for key2, value2 in value1.items():\n",
    "        a[doc_map[key2]][dic_map[key1]] *= idf\n",
    "\n",
    "# Divide each vector with its norm\n",
    "\n",
    "for vec in a:\n",
    "    vec /= norm(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "450aade7-2d40-4649-8139-1477bdbca694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store term-document matrix and mappings as pickle files\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"tdm.pkl\", \"wb\") as f:\n",
    "    pickle.dump(a, f)\n",
    "\n",
    "with open(\"doc_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(doc_map, f)\n",
    "\n",
    "with open(\"dic_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dic_map, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
