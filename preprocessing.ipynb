{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c91229c2-0944-4a05-bf55-63c170ecf4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/emmanuel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "775b1f1c-4d02-495c-bdf8-3126e241cd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Stopword-List.txt', 'r')\n",
    "stop = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f8a1830-4272-4610-bffa-cc294640d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"[\\w]+|[^-_\\w\\s()@#$%^&*+={[\\]};,<>./?~`\\\"]\"\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ac4afc3-be32-48fe-bdf8-c999cc83a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "docs = set()\n",
    "dic = {}\n",
    "punc = ['!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '+', '=', '{', '[', ']', '}', ':', ';', \"'\", '\"', ',', '<', '>', '.', '/', '?', '~', '`']\n",
    "\n",
    "for subdir, dirs, files in os.walk('ResearchPapers'):\n",
    "    for file in files:\n",
    "        with open(subdir + os.sep + file, 'r', encoding='cp1252') as txt:\n",
    "            # Extract document ID from the filename\n",
    "            doc = re.search('[0-9]*', file).group()\n",
    "            doc = int(doc)\n",
    "            docs.add(doc) # Add document ID to the set of docs\n",
    "            # Read and tokenize the text\n",
    "            tokens = tokenize(txt.read())\n",
    "            \n",
    "            # Process each token\n",
    "            for t in tokens:\n",
    "                if t not in stop and t not in punc:\n",
    "                    # Lowercase and stem the token\n",
    "                    term = ps.stem(t.lower())\n",
    "\n",
    "                    # Update the inverted index\n",
    "                    if term not in dic:\n",
    "                        # Create a new linked list (assumed to be defined as `LL` class) for the term\n",
    "                        dic[term] = defaultdict(int)\n",
    "                    dic[term][doc] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e520954-3089-479f-9334-e3281084f83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0, 2: 1, 3: 2, 7: 3, 8: 4, 9: 5, 11: 6, 12: 7, 13: 8, 14: 9, 15: 10, 16: 11, 17: 12, 18: 13, 21: 14, 22: 15, 23: 16, 24: 17, 25: 18, 26: 19}\n"
     ]
    }
   ],
   "source": [
    "doc_list = list(docs)\n",
    "doc_map = dict()\n",
    "for i in range(len(doc_list)):\n",
    "    doc_map[doc_list[i]] = i\n",
    "print(doc_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1aaaaca-6922-42a3-b6a3-c8551878567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_map = dict()\n",
    "keys = sorted(dic.keys())\n",
    "for i in range(len(keys)):\n",
    "    dic_map[keys[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcc93f6b-2302-4698-abc1-481cdc540eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.0\n",
      "27.0\n",
      "12.0\n",
      "89.0\n",
      "30.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "5.0\n",
      "5.0\n",
      "14.0\n",
      "22.0\n",
      "19.0\n",
      "7.0\n",
      "3.0\n",
      "10.0\n",
      "1.0\n",
      "24.0\n",
      "5.0\n",
      "10.0\n",
      "hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {8: 30,\n",
       "             15: 14,\n",
       "             17: 19,\n",
       "             21: 3,\n",
       "             25: 5,\n",
       "             18: 7,\n",
       "             16: 22,\n",
       "             3: 12,\n",
       "             24: 24,\n",
       "             22: 10,\n",
       "             7: 89,\n",
       "             2: 27,\n",
       "             14: 5,\n",
       "             23: 1,\n",
       "             26: 10,\n",
       "             1: 57,\n",
       "             13: 5})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.empty(shape=(len(docs), len(dic)))\n",
    "a.fill(0)\n",
    "for key1, value1 in dic.items():\n",
    "    for key2, value2 in value1.items():\n",
    "        a[doc_map[key2]][dic_map[key1]] = value2\n",
    "\n",
    "for n in doc_list:\n",
    "    print(a[doc_map[n]][dic_map[ps.stem('machine')]])\n",
    "print('hello')\n",
    "dic[ps.stem('machine')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
