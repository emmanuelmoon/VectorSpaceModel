{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c91229c2-0944-4a05-bf55-63c170ecf4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/emmanuel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "775b1f1c-4d02-495c-bdf8-3126e241cd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Stopword-List.txt', 'r')\n",
    "stop = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f8a1830-4272-4610-bffa-cc294640d36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"[\\w]+|[^-_\\w\\s()@#$%^&*+={[\\]};,<>./?~`\\\"]\"\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ac4afc3-be32-48fe-bdf8-c999cc83a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "docs = set()\n",
    "dic = {}\n",
    "punc = ['!', '@', '#', '$', '%', '^', '&', '*', '(', ')', '-', '_', '+', '=', '{', '[', ']', '}', ':', ';', \"'\", '\"', ',', '<', '>', '.', '/', '?', '~', '`']\n",
    "\n",
    "for subdir, dirs, files in os.walk('ResearchPapers'):\n",
    "    for file in files:\n",
    "        with open(subdir + os.sep + file, 'r', encoding='cp1252') as txt:\n",
    "            # Extract document ID from the filename\n",
    "            doc = re.search('[0-9]*', file).group()\n",
    "            doc = int(doc)\n",
    "            docs.add(doc) # Add document ID to the set of docs\n",
    "            # Read and tokenize the text\n",
    "            tokens = tokenize(txt.read())\n",
    "            \n",
    "            # Process each token\n",
    "            for t in tokens:\n",
    "                if t not in stop and t not in punc:\n",
    "                    # Lowercase and stem the token\n",
    "                    term = ps.stem(t.lower())\n",
    "\n",
    "                    # Update the inverted index\n",
    "                    if term not in dic:\n",
    "                        # Create a new linked list (assumed to be defined as `LL` class) for the term\n",
    "                        dic[term] = defaultdict(int)\n",
    "                    dic[term][doc] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3e520954-3089-479f-9334-e3281084f83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0, 2: 1, 3: 2, 7: 3, 8: 4, 9: 5, 11: 6, 12: 7, 13: 8, 14: 9, 15: 10, 16: 11, 17: 12, 18: 13, 21: 14, 22: 15, 23: 16, 24: 17, 25: 18, 26: 19}\n"
     ]
    }
   ],
   "source": [
    "doc_list = list(docs)\n",
    "doc_map = dict()\n",
    "for i in range(len(doc_list)):\n",
    "    doc_map[doc_list[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1aaaaca-6922-42a3-b6a3-c8551878567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_map = dict()\n",
    "keys = sorted(dic.keys())\n",
    "for i in range(len(keys)):\n",
    "    dic_map[keys[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fcc93f6b-2302-4698-abc1-481cdc540eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.empty(shape=(len(docs), len(dic)))\n",
    "a.fill(0)\n",
    "for key1, value1 in dic.items():\n",
    "    for key2, value2 in value1.items():\n",
    "        a[doc_map[key2]][dic_map[key1]] = value2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbafbbfc-3dc0-4c8e-9d10-a5c429307d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.0\n",
      "172.0\n",
      "24.0\n",
      "32.0\n"
     ]
    }
   ],
   "source": [
    "print(a[doc_map[16]][dic_map[ps.stem('machine')]])\n",
    "print(a[doc_map[16]][dic_map[ps.stem('learning')]])\n",
    "print(a[doc_map[24]][dic_map[ps.stem('machine')]])\n",
    "print(a[doc_map[24]][dic_map[ps.stem('learning')]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8709ba3b-7db2-42e5-8b5d-9a57bee31f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197.4727753951736\n",
      "103.55823610675168\n",
      "51.77901346391249\n",
      "309.4932771090597\n",
      "171.13520221872332\n",
      "83.06299242963885\n",
      "53.04713489356411\n",
      "386.1380025196528\n",
      "143.66702660797029\n",
      "143.66702660797029\n",
      "182.28709377504495\n",
      "155.58397289049694\n",
      "179.978187555933\n",
      "88.9540277777064\n",
      "138.57160986403312\n",
      "220.31524131171358\n",
      "42.620195326558964\n",
      "290.29104496372776\n",
      "70.60652486404703\n",
      "324.08527850747424\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from numpy import array\n",
    "from numpy.linalg import norm\n",
    "idf_map = dict()\n",
    "for key1, value1 in dic.items():\n",
    "    idf = math.log10(20/len(value1.keys()))\n",
    "    idf_map[key1] = idf\n",
    "    for key2, value2 in value1.items():\n",
    "        a[doc_map[key2]][dic_map[key1]] *= idf\n",
    "\n",
    "for arr in a:\n",
    "    print(norm(arr))\n",
    "    arr *= 1 / norm(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf946ef9-48b0-44c0-8c34-1c02fb0a5a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07058107428570727\n",
      "0.04575749056067514\n",
      "0.009980357265837654\n",
      "0.05058546989268225\n",
      "0.005835335992085582\n",
      "0.005044040191196953\n"
     ]
    }
   ],
   "source": [
    "print(idf_map[ps.stem('machine')])\n",
    "print(idf_map[ps.stem('learning')])\n",
    "\n",
    "print(a[doc_map[16]][dic_map[ps.stem('machine')]])\n",
    "print(a[doc_map[16]][dic_map[ps.stem('learning')]])\n",
    "print(a[doc_map[24]][dic_map[ps.stem('machine')]])\n",
    "print(a[doc_map[24]][dic_map[ps.stem('learning')]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "450aade7-2d40-4649-8139-1477bdbca694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"tdm.pkl\", \"wb\") as f:\n",
    "    pickle.dump(a, f)\n",
    "\n",
    "with open(\"doc_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(doc_map, f)\n",
    "\n",
    "with open(\"dic_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dic_map, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
